\documentclass[main.tex]{subfiles}

\begin{document}

  \section{Numerical integration}\label{sec:numerical_integration}

  For $(a,b) \in E$ we want to compute to absolute precision say $e^{-D}$ the elementary integrals
  (see Remark \ref{m-rmk:elem_ints})
\begin{align*}
 \int_{-1}^1 \frac{\varphi_{i,j}(u)}{(1-u^2)^{\frac{j}{m}}}  \du.
\end{align*}

In this section, we denote by $α$ the value $1-j/m$, which is the crucial parameter for numerical integration.
Notice that $α=1/2$ for hyperelliptic curves, while for general superelliptic curves $α$ ranges
from $1/m$ to $\frac{m-1}m$ depending on the differential form $\omega_{i,j}$ considered.


As explained in \S \ref{m-subsec:real_mult}, the actual computations will be done on integrals of the form
\begin{equation}
    \label{eq:elem_num_int}
    I_{a,b}(i,j) = \int_{-1}^1\frac{u^{i-1}\du}{(1-u^2)^{\frac jm}\ytab(u)^j}
\end{equation}
so that we restrict the numerical analysis to this case,
the value of elementary integrals being recovered by the polynomial shift
\begin{equation}
    \int_{-1}^1\frac{\varphi_{i,j}(u)}{(1-u^2)^{\frac jm}}\du
    = \sum_{l=0}^{i-1} {i-1 \choose l} \left(\frac{b+a}{b-a}\right)^{i-1-l} I_{a,b}(l,j).
\end{equation}

We study here two numerical integration schemes:
\begin{itemize}
    \item 
the double-exponential change of variables is completely general and its robustness
allows to compute rigorously all integrals of periods in a very unified setting
even with different values of $\alpha$;
\item in the special case of hyperelliptic curves however, the Gauss-Chebychev method applies and
    provides a better scheme (fewer and simpler integration points).
\end{itemize}
For larger $m$, the periods could also be computed using general Gauss-Jacobi integration
of parameters $\alpha,\alpha$. However a different scheme has to be computed for each $\alpha$,
and it now involves computing roots of general Jacobi polynomials to large accuracy, which
makes it hard to compete with the double-exponential scheme.

\subsection{Double-exponential integration}\label{sec:de_int}

Using the double-exponential change of variable
\begin{equation}
    \label{eq:de_change}
u=\tanh(λ\sinh(t)),
\end{equation}
the singularities at $\pm1$ are pushed to infinity and
the integral \eqref{m-eq:elem_num_int} becomes
\begin{equation}
    I_{a,b}(i,j) = \int_\R g(t)\dt
\end{equation}
with
\begin{equation}
   % g(t) = \frac{u(t)^{i-1}}{\ytab(u(t))^j}\frac{λ\cosh(t)}{\cosh(λ\sinh(t))^{2α}}
   g(t) = \frac{u(t)^{i-1}}{\ytab(u(t))^j}\frac{λ\cosh(t)}{\cosh(λ\sinh(t))^{2α}}
\end{equation}

Let
\begin{equation}
    Z_r = \set{\tanh(λ\sinh(z)), -r<\Im(z)<r }
\end{equation}
be the image of the strip $\Delta_r$ under the change of
variable \eqref{eq:de_change}.

Since we can compute the distance of each branch point $u_i$ to
both $[-1,1]$ and its neighborhood $Z_r$ (see \S \ref{m-sec:dist_zr}), we obtain
  \begin{lemma}
      There exist explicitly computable
      constants $M_1$, $M_2$ such
      that
      \begin{itemize}
          \item for $u\in[-1,1]$, $\abs{\frac{u^{i-1}}{\ytab(u)^{j}}}\leq M_1$
          \item for $u\in Z_r$, $\abs{\frac{u^{i-1}}{\ytab(u)^{j}}}\leq M_2$.
      \end{itemize}
  \end{lemma}

We also introduce the following quantities
\begin{align}
    X_r &=\cos(r)\sqrt{\frac{π}{2λ\sin r}-1} \\
    B(r,α) &=
    \frac{2}{\cos r}
    \left(
        \frac{X_r}2(\frac1{\cos^{2α}(λ\sin r)}+\frac1{X_r^{2α}})
        +\frac{1}{2α\sinh^{2α}X_r}
    \right)
\end{align}

Once computed the two bounds $M_1$, $M_2$ and the constant $B(r,α)$,
we obtain a rigorous integration scheme as follows
\begin{thm}
    With the notations above, for all $D>0$, choose $h$ and $n$ such that
    \begin{equation}
        \begin{cases}
            h \le \frac{2πr}{D + \log(2M_2 B(r,α) + e^{-D})}\\
            nh \ge \asinh(\frac{D+\log(\frac{2^{2α+1}M_1}{α})}{2αλ})
        \end{cases}
    \end{equation}
    then
    \begin{equation}
        \abs{
            I_{a,b}(i,j)
            - h\sum_{k=-n}^n
            w_k \frac{u_k^{i-1}}{\ytab(u_k)^j}
        } \leq e^{-D}
    \end{equation}
    where
    \begin{equation}
        \begin{cases}
            u_k = \tanh(λ\sinh(kh))\\
            w_k = \frac{λ\cosh(kh)}{\cosh^{2α}(λ\sinh(kh)}
        \end{cases}
    \end{equation}
\end{thm}

The proof follows the same lines as the one in \cite{Molin2010}:
we write the Poisson formula on $h\Z$ for the function $g$
\begin{equation}
    \underbrace{h\sum_{\abs{k}>n}g(kh)}_{e_T}
 + h\sum_{k=-n}^n g(kh)
 = \int_\R g
 +
     \underbrace{\sum_{k\in\Z^\ast} \hat g(\frac{k}{h})}_{e_Q}
\end{equation}
and control both error terms $e_T$ and $e_q$ by lemma \ref{m-lem:de_error_trunc}
and \ref{m-lem:de_error_quad} below. The actual parameters $h$ and $n$ follow
by bounding each error by $e^{-D}/2$.

\begin{lemma}[truncation error]
    \label{lem:de_error_trunc}
    \begin{equation}
        \sum_{\abs{k}>n}\abs{hg(kh)}
        \leq \frac{2^{2α} M_1}{αλ}\exp(-2αλ\sinh(nh))
    \end{equation}
\end{lemma}
\begin{proof}
    We bound the sum by the integral of a decreasing function
    \begin{align*}
        \sum_{\abs{k}>n}\abs{hg(kh)}
        &\leq2M_1\int_{nh}^\infty\frac{λ\cosh(t)}{\cosh(λ\sinh(t))^{2α}}
        =2M_1\int_{λ\sinh(nh)}^\infty\frac{\dt}{\cosh(t)^{2α}}\\
        &\leq 2^{2α+1} M_1\int_{λ\sinh(nh)}^\infty e^{-2αt}\dt
        = \frac{2^{2α} M_1}{α}e^{-2αλ\sinh(nh)}
    \end{align*}
\end{proof}

\begin{lemma}[discretization error]
    \label{lem:de_error_quad}
    With the current notations,
    \begin{equation}
        \sum_{k\neq0}\abs{\hat g(\frac kh)}
        \leq
        \frac{M_2B(r,α)}{e^{2πr/h}-1}.
    \end{equation}
\end{lemma}


We first bound the Fourier transform by a shift of contour
\begin{equation}
    \forall X>0, \hat g(\pm X) = e^{-2πXr} \int_{\R} g(t\mp ir) e^{-2iπtX}\dt
\end{equation}
so that
\begin{equation}
    \sum_k \abs{\hat g(\frac kh)}
    \leq
    \frac{2M_2}{e^{2πr/h}-1}\int_\R \abs{
    \frac{λ\cosh(t+ir)}{\cosh(λ\sinh(t+ir))^{2α}}}\dt
\end{equation}

Now the point $λ\sinh(t+ir) = X(t)+iY(t)$ lies on the hyperbola
$Y^2 =λ^2(\sin^2r+\tan^2 rX^2)$, and
\begin{align}
    \abs{λ\cosh(t+ir)} &\leq λ\cosh(t) =\frac{X'(t)}{\cos(r)}\\
    \abs{\cosh(X+iY)}^2 &= \sinh(X)^2+\cos(Y)^2
\end{align}
so that
\begin{equation}
    \int_\R \abs{
    \frac{λ\cosh(t+ir)}{\cosh(λ\sinh(t+ir))^{2α}}}\dt
    \leq
    \frac{2}{\cos r}\int_0^\infty\frac{\d X}{(\sinh(X)^2+\cos(Y)^2)^α}
\end{equation}
For $X_0=0$, $Y_0=λ\sin r<\frac{π}2$, and $Y_r=\frac{π}2$ for
$X_r=\cos(r)\sqrt{\frac{π}{2Y_0}-1}$.

  We cut the integral at $X=X_r$ and write
  \begin{align}
      \int_0^{X_r}\frac{\d X}{(\sinh(X)^2+\cos(Y)^2)^α}
      & \leq \int_0^{X_r}\frac{\d X}{(X^2+\cos^2Y)^α} \\
      \int_{X_r}^\infty\frac{\d X}{(\sinh(X)^2+\cos(Y)^2)^α}
      & \leq \int_{X_r}^\infty\frac{\d X}{(\sinh X)^{2α}}
  \end{align}

  We bound the first integral by convexity:
  since $Y(X)$ is convex and $\cos$ is concave decreasing for $Y\leq Y_r$ we
  obtain by concavity of the composition
  \begin{equation}
      \forall X\leq X_r, \cos(Y)\geq \cos(Y_0)(1-\frac{X}{X_r})
  \end{equation}
  Now $X^2+\cos^2Y\geq P_2(X)$ where
  \begin{equation}
     P_2(X) = X^2(1+\frac{\cos^2(Y_0)}{X_r^2})-2\frac{\cos^2(Y_0)}{X_r}X+\cos^2(Y_0)
  \end{equation}
  is a convex quadratic, so $X\mapsto P_2(X)^{-α}$ is still convex and the integral
  is bounded by one trapeze
  \begin{equation}
      \int_0^{X_r}\frac{\d X}{P_2(X)^α}\leq X_r\frac{P_2(0)+P_2(X_r)}2
      = \frac{X_r}2\left(\frac1{\cos^{2α}(Y_0)}+\frac1{X_r^{2α}}\right)
  \end{equation}

  For the second integral we use
  $\sinh(X)\geq\sinh(X_r)e^{X-X_r}$ to obtain
  \begin{equation}
      \int_{X_r}^\infty \frac{\d X}{\sinh(X)^{2α}} \leq \frac1{2α\sinh(X_r)^{2α}}
  \end{equation}

\subsection{Gauss-Chebychev integration}
\label{sub:gauss_chebychev_integration}

In the case of hyperelliptic curves, we have $α=\frac12$ and the integral
\begin{equation}
    \int_{-1}^1\frac{\varphi_{i,j}(u)}{\sqrt{1-u^2}}\du
\end{equation}
can be efficiently handled by Gaussian integration with weight
$1/\sqrt{1-u^2}$,
for which the corresponding orthogonal polynomials are
Chebychev polynomials.

In this case, the integration formula is particularly
simple: there is no need to actually compute the Chebychev polynomials
since their roots are explicitly given as cosine functions.
\begin{thm}[Gauss-Chebychev integration]
    Let $g$ be holomorphic around $[-1,1]$. Then for all
    $n$, there exists $\xi \in ]-1,1[$ such that
    \begin{equation}
        \label{eq:gauss_chebychev}
        \int_{-1}^1\frac{g(u)}{\sqrt{1-u^2}}\du
        - \sum_{k=1}^n w_k g(u_k)
        = \frac{π2^{2n+1}}{2^{4n}}\frac{g^{(2n)}(\xi)}{(2n)!}
     = E(n),
    \end{equation}
    with constant weights $w_k = w =\frac{π}n$ and nodes $u_k = \cos(\frac{2k-1}{2n}π)$.
\end{thm}

Moreover, very nice estimates on the error $E_n$ can by obtained by applying the residue
theorem on an ellipse $ε_r$ of the form
\begin{equation}
    ε_r = \set{z, \abs{z-1}+\abs{z+1} = 2r }
\end{equation}

  \begin{figure}[H] \begin{center}
      \includegraphics[width=5cm,page=1]{images/ellipse.pdf}
  \end{center} \caption{Ellipse parameters.}
  \label{fig:ellipse} \end{figure}

\begin{thm}[\cite{ChawlaJain68},Theorem 5]
    Let $r>1$ such that $g$ is holomorphic on $ε_r$. Then
    the error in \eqref{eq:gauss_chebychev} satisfies
    \begin{equation}
        \abs{E(n)}\leq \frac{2πM(r)}{(r+\sqrt{r^2-1})^{2n}-1}
    \end{equation}
    where $M(r)=\max\set{\abs{f(z)},z\in ε_r}$.
\end{thm}

Now we use this theorem with a function
$g_{i,j}(u)=\frac{u^i}{\sqrt{Q(u)}}$ for an explicitely factored
polynomial $Q(u)=\prod(u-u_k)$, so that the error can be
explicitly controlled.

\begin{lemma}
    \label{lem:param_r}
    Let $r>1$ be such that $2r<\abs{u_i-1}+\abs{u_i+1}$ for all
    roots $u_i$ of $Q$,
    then there exists an explicitly computable
    constant $M(r)$ such that for all $u\in ε_r$
    \begin{equation}
        \abs{\frac{u^{i-1}}{\ytab(u)}}\leq M(r)
    \end{equation}
\end{lemma}
\begin{proof}
We simply compute the distance
        $d_r(u_i) = \inf_{z\in ε_r}\abs{z-u_i}$
 from a root $u_i$ to the ellipse $ε_r$, and let
 $M(r) =  \frac{r^{i-1}}{\sqrt{\prod d_r(u_i)} }$.
\end{proof}

\begin{prop}
    With $r$ and $M(r)$ satisfying lemme \ref{m-lem:param_r},
    for all $n$ such that
    \begin{equation}
        n \geq \frac{D+\log(2πM(r))+1}{2\log(r+\sqrt{r^2-1})}
    \end{equation}
    we have
    \begin{equation}
        \abs{I_{a,b}(i,j)
        - \frac{π}n\sum_{k=1}^n \frac{u_k^{i-1}}{\ytab(u_k)}}
            \leq e^{-D}
    \end{equation}
    where $u_k=\cos(\frac{2k-1}{2n}π)$.
\end{prop}

More details on the choice of $r$ and the computation of $M(r)$
are given in section \ref{m-choice_r_ellipse}.

\iffalse
the $2n$-th derivative can be
estimated quite precisely by Cauchy formula.

\newcommand{\rmax}{r_{\mathrm{max}}}
\begin{lemma}
    Let $u_i$ be the roots of $Q$, and $r_i$ the distance from
    $u_i$ to $[-1,1]$. Let also $\rmax$ be the minimum of $r_i$.

    Then for all $r<\rmax$,
    \begin{equation}
    \abs{E_n} \leq \frac{2π}rB(r)(2r)^{-2n}
    \end{equation}
    where
    \begin{equation}
        B(r) = \frac1{\prod_i(r_i-r)}
    \end{equation}
\end{lemma}

For fixed $n$, assuming the minimum value $\rmax$ is attained for exactly
$m$ roots $u_i$, we can optimize the choice of $r$ by writing
$r=\rmax(1-λ)$ so that $B(r)\leq\frac{B_1}{(\rmaxλ)^m}$, where
$B_1=\prod(r_i-\rmax)$, the product being on those $r_i>\rmax$.

Then
\begin{equation}
    \frac{2π}rB(r)(2r)^{-2n}
    = \frac{2πB_1}{\rmax^m} λ^{-m}(2\rmax(1-λ))^{-2n-1}
\end{equation}
and $λ^m(1-λ)^{2n+1}$ is maximum for
\begin{equation}
    λ = \frac{m}{2n+1+m}
\end{equation}
in which case
$λ^m(1-λ)^{2n+1}=(1+\frac{2n+1}m)^{-m}(1+\frac{m}{2n+1})^{-(2n+1)}$.

We therefore consider this value of $λ$, so that
\begin{equation}
    E(n) \leq
    \frac{2πB_1}{\rmax^m}
    (1+\frac{2n+1}m)^m(1+\frac{m}{2n+1})^{2n+1}(2\rmax)^{-2n-1}
\end{equation}

  \begin{align}
      \sinh(x+iy) &= \sinh x\cos y+i\cosh x\sin y\\
      \cosh(x+iy) &= \cosh x\cos y+i\sinh x\sin y
  \end{align}
  so that writing $λ\sinh(t+ir)=X+iY$ we express the integral in terms
  of $X,Y$ with
  \begin{align}
      X &= λ\sinh t\cos r\\
      Y &= λ\cosh t\sin r\\
      Y^2 =λ^2(\sin^2r+\tan^2 rX^2)\\
      λ\cosh(t+ir) &= λ\cosh t\cos r+iλ\sinh t\sin r \\
                     &= Y/\tan(r) + i X\tan(r)\\
      \abs{\cosh(X+iY)}^2
      &= \cosh^2 X\cos^2 Y+\sinh^2 X\sin^2 Y \label{eq:boundchcos}\\
      &= \sinh^2 X + \cos^2 Y \label{eq:boundchsh}
  \end{align}
\fi

\iffalse
assume(t,real);
assume(tau,real);
assume(l,real);
assume(l>0);
assume(l<Pi/2);
phi:=tanh(l*sinh(t+I*tau));;
simplify(expand(diff(phi,t$1)));
\fi


\biblio
\end{document}
