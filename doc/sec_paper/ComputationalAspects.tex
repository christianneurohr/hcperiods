\documentclass[main.tex]{subfiles}

\begin{document}

  \section{Computational aspects}\label{sec:comp_asp}

   \subsection{Complexity analysis}

   We recall the parameters of the problem: we consider a superelliptic curve $\cu$ given by
   $\caff:y^m=f(x)$ with $f \in \C[x]$ separable of degree $n$. The genus $g$ of $\cu$ satisfies
   $$g \leq \frac{(m-1)(n-1)}2=O(mn).$$

   Let $D$ be some desired accuracy (a number of decimal digits). The computation of
   the Abel-Jacobi map on $\cu$ has been decomposed into the
   following list of tasks:
   \begin{enumerate}
       \item computing the $(n-1)$ vectors of elementary integrals,
       \item computing the big period matrix $\Omega=(\OA,\OB)$ \eqref{m-eq:OAOB},
       \item computing the small period matrix $\tau = \OA^{-1}\OB$ \eqref{m-eq:tau},
       \item evaluating the Abel-Jacobi map at a point $P \in \cu$,
   \end{enumerate}
   all of these to absolute precision $D$.

   Let $N(D)$ be the number of points of numerical integration.
   If $m=2$, we have
   $N(D)=O(D)$ using Gauss-Chebychev integration, while $N(D)=O(D\log D)$
   via double-exponential integration.

   For multiprecision numbers, we consider (see \cite{BrentZimmermann}) that the multiplication has
   complexity $\cmul(D)=O(D \log^{1+\varepsilon}D)$,
   while simple transcendental functions (log, exp, tanh, sinh,\dots) can be evaluated
   in complexity $\ctrig(D)=O(D\log^{2+\varepsilon} D$). For complex $m$-th roots we also consider the complexity $\ctrig(D)$ using
   $\exp(\frac{1}{m}\log(\cdot))$.
    Moreover, we assume that multiplication of a $g \times g$ matrix can
    be done using $O(g^{2.8})$ multiplications.

   \subsubsection{Computation of elementary integrals}
   \label{sec:comp_elem}

   For each elementary cycle $\gamma_e\in \Gamma$, we numerically evaluate the vector of $g$
   elementary integrals from \eqref{m-eq:elem_num_int} as sums of the form
   \begin{equation*}
       I_{a,b} \approx \sum_{\ell=1}^N w_\ell\frac{u_\ell^{i-1}}{y_\ell^j},
   \end{equation*}
   where $N = N(D)$ is the number of integration points, $w_\ell,u_\ell$ are integration weights and points,
   and $y_\ell=\ytab(u_\ell)$.

   We proceed as follows:
   \begin{itemize}
   \item for each $\ell$, we evaluate the abscissa and weight $u_\ell,w_\ell$ using
       a few \footnote{this can be reduced to evaluating
           a few multiplications and at most one exponential.} trigonometric or hyperbolic functions,
   \item we compute $y_\ell=\ytab(u_\ell)$ using $n-2$ multiplications and one $m$-th root,
       as shown in \S \ref{m-subsec:computing_roots} below;
   \item starting from $\frac{w_\ell}{y_\ell}$, we evaluate all $g$ terms $w_\ell\frac{u_\ell^{i-1}}{y_\ell^j}$
       each time either multiplying by $u_\ell$ or by $\frac{1}{y_\ell}$, and add each to the corresponding
       integral.
   \end{itemize}

   Altogether, the computation of one vector of elementary integrals takes
   \begin{equation}\label{eq:elem_int_complexity}
    \ctot(D) = N(D)\ctrig(D)+N(D)(n-2+\log D)\cmul(D)+N(D)g\cmul(D)
   \end{equation}
    operations,
   so that depending on the integration scheme we obtain:
   \begin{thm}\label{thm:complexity_integrals}
       Each of the $(n-1)$ elementary vector integrals can be computed to precision $D$ using
       \begin{equation*}
           O(N(D)\cmul(D)(g+\log D)) =
           \begin{cases}
               O(D^2\log^{1+\varepsilon} D (g + \log D)) \text{ operations, if $m=2$,}\\
               O(D^2\log^{2+\varepsilon} D (g + \log D)) \text{ operations, if $m>2$.}
           \end{cases}
       \end{equation*}
   \end{thm}
   %\begin{proof}
   %  Plugging in $N(D),T(D),M(D)$ in equation \eqref{m-eq:elem_int_complexity} and using that $n-1 = O(g)$.
   %\end{proof}

   \subsubsection{Big period matrix}

   One of the nice aspects of the method is that we never compute
   the dense matrix $\OC\in\C^{g\times 2g}$ from Section \ref{sec:strat_pm}, but
   keep the decomposition of periods in terms of the elementary integrals
   $\int_{\gamma_e}\omega_{i,j}$ in $\C^{g\times (n-1)}$.

   Using the symplectic base change matrix $S$ introduced
   in \S \ref{m-subsec:symp_basis}, the symplectic homology basis is given
   by cycles of the form
   \begin{equation}
       \label{eq:base_change_cycles}
       \alpha_i = \sum_{\substack{e \in E \\ l\in\Z/m\Z}} s_{e,l}\gamma_e^{(l)}
   \end{equation}
   where $\gamma_e^{(l)} \in \Gamma$ is a generating cycle
   and $s_{e,l}\in\Z$ is the corresponding entry of $S$.

   We use \eqref{m-eq:periods} to compute the coefficients of the big period
   matrix $(\OA,\OB)$, so that each term of \eqref{m-eq:base_change_cycles}
   involves only a fixed number of multiplications.

   In practice, these sums are sparse and their coefficients are very small integers
   (less than $m$), so that the change of basis is performed using
   $O(g^3D\log^{1+\varepsilon}D)$ operations
   (each of the $O(g^2)$ periods is a linear combination of $O(g)$ elementary integrals,
   the coefficients involving precision $D$ roots of unity).

   However, we have no proof of this fact and in general the symplectic reduction
   could produce dense base change with coefficients of size $O(g)$,
   so that we state the following far from optimal result.
   \begin{thm}
       Given the $(n-1)\times g$ elementary integrals to precision $D$,
       we compute the big period matrix using $O(g^3(D+g)\log^{1+\varepsilon}(D+g))$ operations.
   \end{thm}

   \subsubsection{Small period matrix}

   Finally, the small period matrix is obtained by solving $\OA\tau=\OB$,
   which can be done using $O(g^{2.8})$ multiplications.

   \subsubsection{Abel-Jacobi map}

  This part of the complexity analysis is based on the results of Section \ref{m-sec:comp_ajm} and assumes that we have already computed a big period matrix and all related data.

  Let $\ctot(D)$ be the number of operations needed to compute a vector of $g$ elementary integrals  (see \eqref{m-eq:elem_int_complexity}). The complexity class of $\ctot(D)$ in $O$-notation is given in
  Theorem \ref{m-thm:complexity_integrals}.

   \begin{thm} \
   \begin{itemize}
     \item[{\upshape{(i)}}] For each finite point $P \in \caff$ we can compute $\int_{P_0}^P \bar\w$ to precision $D$ using
      $\ctot(D)$ operations.
     \item[{\upshape{(ii)}}] For each infinite point $P_{\infty} \in \cu$ we can compute a representative of $\int_{P_0}^{P_{\infty}} \bar\w \mod \Lambda$ to precision $D$ using
      \begin{itemize}
       \item[$\bullet$] $n$ vector additions in $\C^g$, if $\delta = \gcd(m,n) = 1$,
       \item[$\bullet$] $n \ctot(D)$ operations in the case of Theorem \ref{m-thm:ajm_inf_ord1},
       \item[$\bullet$] $n(n+\frac{m}\delta)\ctot(D)$ operations  in the case of Theorem \ref{m-thm:ajm_inf_ordgt1}.
      \end{itemize}
      \item[{\upshape{(iii)}}] Reducing a vector $v \in \C^g$ modulo $\Lambda$ can be done using $O(g^{2.8})$ multiplications.
    \end{itemize}
  \end{thm}
   \begin{proof}
    \begin{itemize}
     \item[(i)] Follows from combining the results from \S \ref{m-subsec:ajm_ram_pts} and Remark \ref{m-rmk:ajm_finite_int}.
     \item[(ii)] The statements follow immediately from \S \ref{m-subsec:ajm_inf_cop}, Theorem \ref{m-thm:ajm_inf_ord1} and Theorem \ref{m-thm:ajm_inf_ordgt1}.
     \item[(iii)] By \S \ref{m-subsec:lat_red}, the reduction modulo the period lattice requires one $2g \times 2g$ matrix inversion and one multiplication.
    \end{itemize}
   \end{proof}

   \subsection{Precision issues}

   As explained in \S \ref{subsec:arb}, the ball arithmetic model allows
   to certify that the results returned by the Arb program \cite{Johansson2013arb} are correct.
   It does not guarantee that the result actually achieves the desired
   precision.

   As a matter of fact, we cannot prove a priori that bad accuracy loss
   will not occur while summing numerical integration terms or
   during matrix inversion.

   However, we take into account all predictable loss of precision:
   \begin{itemize}
       \item While computing the periods using equations \eqref{eq:periods}
           and \eqref{eq:polshift}, we compute a sum with coefficients
           \begin{equation*}
               C_{a,b}^{-j} \left(\frac{b-a}{2}\right)^i
               {i-1 \choose l} \left(\frac{b+a}{b-a}\right)^{i-1-l}
           \end{equation*}
           whose magnitude can be controlled a priori. It has size $O(g)$.
       \item The size of the coefficients of the symplectic reduction
           matrix are tiny (less than $m$ in practice), but we can take
           their size into account before entering the numerical steps.
           Notice that generic HNF estimates lead to a very pessimistic
           estimate of size $O(g)$ coefficients.
       \item Matrix inversion of size $g$ needs $O(g)$ extra bits.
   \end{itemize}
   This leads to increasing the internal precision from $D$ to $D + O(g)$,
   the implied constant depending on the configuration of branch points.

   \begin{rmk}
   In case the end result is imprecise by $d$ bits, the user simply needs to
   run another instance to precision $D+d$ to reach the desired accuracy.

   In fact, the mathematical quantities and the sequence of arithmetic
   operations performed in the algorithm remain the same. Now
   if the absolute error is reduced by $d$ bits on input of an elementary
   operation this remains true on output; by induction this is true for
   the final result.
   \end{rmk}

   \subsection{Integration parameters}

   \subsubsection{Gauss-Chebychev case}

   Recall from \S \ref{m-subsec:gauss_chebychev_integration} that we can parametrize the ellipse $ε_r$ via
   $ε_r = \set{ \cosh(r+it) = \cos(t-ir), t\in]-π,π] }$.

   \begin{figure}[H]
       \begin{center}
       \includegraphics[width=5cm,page=3]{images/ellipse.pdf}
   \end{center} \caption{ellipse parameters.}
   \label{fig:ellipse2}
   \end{figure}

   The sum of its semi-axes is $e^{r}$
   and one needs
   \[
       N \geq \frac{D+\log(2πM(r)+e^{-D})}{2r}
   \]
   to have $\abs{E(N)}\leq e^{-D}$.

   The distance $d_k=\dist(u_k,ε_r)$ from a branch point $u_k \in U^+ \cup U^-$
   to the ellipse $ε_r$ can be computed
   applying Newton's method to the scalar product function
   $s(t) = \Re(\overline{z'}(u_k-z))$, where $z = \cos(t-ir)$ and
   we take $t=\Re(\arccos(u_k))$ as a starting point (see Figure \ref{fig:ellipse2}).
   By convexity of the ellipse,
   the solution is unique on the quadrant containing $u_k$.

   \paragraph{Choice of $r$}\label{par:gc_int_r}

   Let $\abs{u_k-1}+\abs{u_k+1}=2\cosh(r_k)$. We need to choose
   $r<r_0=\min_k r_k$ (so that $u_k\not\in ε_r$) in order to minimize
   the number of integration points \eqref{m-eq:Ngc}. We first
   estimate how the bound $M(r)$ varies for $r<r_0$.
   \begin{itemize}
       \item
   For all $k$ such that $r_k > r_0$, we compute
   explicitly the distance $d_k=\dist(u_k,ε_{r_0})<\dist(u_k,ε_r)$.
   \item
   For all $k$ such that $r_k=r_0$, we use first order approximation
   $\dist(u_k,Z_{r-η}) = η D_k + O(η^2)$,
   where $D_k = \abs{\frac{\partial u_k}{\partial r_k}} = \abs{\sin(t_k-ir_k)}$.
   \end{itemize}

   Let $K$ be the number of branch points $u_k \in U^+ \cup U^-$ such that $r_k=r_0$ and
   \[ M_0 = \sqrt{\prod_{r_k = r_0} D_k\prod_{r_k>r_0}d_k}^{-1}, \]
   then the integrand is bounded on $ε_{r_0-η}$ by
   \[ M(r_0-η) = M_0 \sqrt{η}^{-K} (1+O(η)). \]
   Plugging this into \eqref{eq:Ngc}, the number of integration points
   satisfies
   \[
       2N = \frac{D+\log(2πM_0) - K/2 \log(η) }{r_0-η}(1+O(η)).
   \]

   The main term is minimized for $η$ satisfying
   $η\left(2\frac{D+\log(2πM_0)}K+1-\log(η)\right)=r_0$. The solution
   can be written as a Lambert function or we use
   the approximation
   \[ r = r_0 - η = r_0 \left( 1 - \frac{1}{A+\log\frac{A}{r_0}} \right), \]
   where $A = 1+\frac2K(D+\log(2πM_0))$.

   \subsubsection{Double-exponential case}\label{subsec:de_case}

   For the double-exponential integration (\S \ref{m-subsec:de_int})
   we use the parametrization
   $$\partial Z_r = \set{ z = \tanh(λ\sinh(t+ir)), t\in\R }$$ to compute
   the distance from a branch point $u_k \in U^+ \cup U^-$ to $Z_r$ by Newton's method
   as before.
   %$f(t) = \Re(\overline{z'}(u_k-z))$, where $z = \tanh(λ\sinh(t+ir))$
   %and starting point $t=\Re(z^{-1}(p))$.

   Unfortunately, the solution may not be unique, so once
   the parameter $r<r_0$ is chosen (see below), we use ball arithmetic to compute a rigorous
   bound of the integrand on the boundary of $Z_r$. The process consists in
   recursively subdividing the interval until the images of the subintervals by the
   integrand form an $ε$-covering.

   \paragraph{Choice of $r$}

   We adapt the method used for Gauss-Chebychev. This time the number $N$ of integration
   points is obtained from equation \eqref{m-eq:de_parameters}.

   Writing $u_k = \tanh(λ\sinh(t_k+ir_k))$, we must choose
   $r<r_0=\min_k \{r_k\}$ to ensure $u_k\not\in Z_r$. Let
   \[ M_0 = (\prod_{r_k = r_0} D_k\prod_{r_k>r_0}d_k)^{-j/m} \]
   where $d_k=\dist(u_k,Z_{r_0})<\dist(u_k,Z_r)$ and
   \[ D_k = \abs{\frac{\partial u_k}{\partial r_k}} = \abs{\frac{λ \cosh(t_k+ir_k)}{\cosh(λ\sinh(t_k+ir_k))^2}} \]
   is such that $\dist(u_k,Z_{r-η}) = η D_k + O(η^2)$, then
   the integrand is bounded on $Z_{r_0-η}$ by
   \[ M_2 = M_0 η^{-\frac{jK}m} (1+O(η)). \]
   Then
   \[ h = \frac{2π(r_0-η)}{D+\log(2B(r_0,α)M_0)-jK/m\log(η)}+O(η) \]
   and the maximum is obtained for the solution $η$ of $η(A-\log η)=r_0$
   where $A=1+\frac{m}{jK}(D+\log(2B(r_0,α)M_0))$.

   \subsection{Implementation tricks}

   Here we simply give some ideas that we used in our implementation(s) to improve constant factors hidden in the big-$O$ notation, i.e. the absolute running time.

   In practice, 80 to 90\% of the running time is spent on numerical integration
   of integrals \eqref{m-eq:periods}. According to \S\ref{m-sec:comp_elem},
   for each integration point $u_\ell\in]-1,1[$ one first evaluates the $y$-value
   $y_\ell=\ytab(u_\ell)$, then adds the contributions $w_\ell\frac{u_\ell^i}{y_\ell^j}$ to
   the integral of each of the $g$ differential forms.

   We shall improve on these two aspects, the former being prominent for hyperelliptic curves,
   and the latter when the $g \gg n$.

    \subsubsection{Computing products of complex roots}\label{subsec:computing_roots}

    Following our definition \eqref{m-eq:def_yab}, computing $\ytab(u_\ell)$ involves
    $(n-2)$ $m$-th roots for each integration point.

    Instead, we fall back to one single (usual) $m$-th root
    by computing $q(u)\in\frac12\Z$ such that
  \begin{equation}
      \label{eq:ytab_comp}
      \ytab(u) = \zeta^{q(u)} \Big( \prod_{u_k\in U^-}(u-u_k) \prod_{u_k\in U^+} (u_k-u) \Big)\mr.
  \end{equation}
  This can be done by tracking
  the winding number of the product while staying away from the branch cut
  of the $m$-th root.
  For complex numbers $z_1,z_2 \in \C$ we can make a diagram of
  $\frac{\sqrt[m]{z_1}\sqrt[m]{z_2}}{\sqrt[m]{z_1z_2}} \in \{ 1, \zeta,
  \zeta^{-1} \}$, depending on the position of $z_1,z_2$ and their product
  $z_1z_2$ in the complex plane, resulting in the following lemma:

  \begin{lemma}\label{lemma:wind_numb}
  Let $z_1,z_2 \in \C  \setminus  ]\infty,0]$. Then,
  $$\frac{\sqrt[m]{z_1}\sqrt[m]{z_2}}{\sqrt[m]{z_1z_2}} = \begin{cases}
                                                           \zeta, \quad \text{if} \quad \Im(z_1), \Im(z_2) > 0 \quad \text{and} \quad \Im(z_1z_2) < 0 , \\
                                                           \zeta^{-1}, \text{if} \quad \Im(z_1), \Im(z_2) < 0 \quad \text{and} \quad \Im(z_1z_2) > 0 , \\
                                                           1, \quad \text{otherwise}.
                                                         \end{cases}$$
   For $z \in ]\infty,0]$ we use $\sqrt[m]{z} = \zeta^{\frac{1}{2}} \cdot \sqrt[m]{-z}$.
  \end{lemma}
  \begin{proof}
   Follows from the choices for $\sqrt[m]{\cdot}$ and $\zeta$ that were made in \S \ref{m-subsec:roots_branches}.
  \end{proof}
  Lemma \ref{m-lemma:wind_numb} can easily be turned into an algorithm that computes $q(u)$.

   \subsubsection{Doing real multiplications}\label{subsec:real_mult}

   Another possible bottleneck comes from the multiplication by the numerator
   $u_\ell$, which is usually done $g-m-1$ times for each of
   the $N$ integration points (more precisely, as we saw in the proof of Proposition \ref{m-prop:holom_diff}, for each exponent $j$
   we use the exponents $0\leq i\leq n_i = \floor{\frac{nj-δ}m}$, with $\sum n_i = g$).

   Without polynomial shift \eqref{m-eq:polshift}, this numerator would be
   $x_\ell=u_\ell+\frac{b+a}{b-a}$. However, $x_\ell$ is a complex number while $u_\ell$
   is real, so computing with $u_\ell$ saves a factor almost $2$ on this aspect.

   \subsection{Further ideas}

   \subsubsection{Improving branch points}
   \label{subsec:improving}

   As we saw in Section \ref{m-sec:numerical_integration}, the number of integration points
   closely depends on the configuration of branch points.

   In practice, when using double-exponential integration, the constant $r$ is usually bigger than $0.5$
   for random points, but we can exhibit bad configurations with $r\approx 0.1$.
   In this case however, we can perform a change of coordinate by a Moebius transform
   $x\mapsto \frac{ax+b}{cx+d}$, as explained in Remark \ref{m-rmk:moebius}, to redistribute the points more evenly.

   Improving $r$ from $0.1$ to say $0.6$ immediately saves a factor $6$ on the running time.

   \subsubsection{Near-optimal tree}
    As explained in \S \ref{m-subsec:cycles_homo} we integrate along the edges of a maximal-flow spanning tree $T = (X,E)$, where the capacity $r_e$ of an edge $e = (a,b) \in E$ is computed as
    \begin{equation*}
     r_e =  \min_{c \in X \setminus \{ a,b\}} \begin{cases}
        % \frac{\abs{c-a}+\abs{c-b}}{\abs{b-a}}, \text{ if $m=2$,}\\
         \abs{ \Im(\sinh^{-1}(\tanh^{-1}(\frac{2c - b -a}{b-a})/\lambda) }, \text{ if $m>2$.}
            \end{cases}
    \end{equation*}
    Although this can be done in low precision, computing $r_e$ for all $(n-1)(n-2)/2$ edges of the complete graph
    requires $O(n^3)$ evaluations of elementary costs (involving transcendental functions if $m>2$).

    For large values of $n$ (comparable to the precision), the computation of
    these capacities has a noticeable impact on the running time. This can be
    avoided by computing a \emph{minimal spanning tree} that uses the euclidean
    distance between the end points of an edge as capacity, i.e.
    $r_e = \abs{b-a}$, which reduces the complexity to $O(n^2)$
    multiplications.

    Given sufficiently many branch points that are randomly distributed in the
    complex plane, the shortest edges of the complete graph tend to agree with
    the edges that are well suited for integration.
    %Heuristically we find that starting with $n \approx 15$ this approach
    %becomes favourable.
    % -> Pascal disagrees here, spanning tree ~ 1% of the running time for prec=128 and n=31...

    \subsubsection{Taking advantage of rational equation}

    In case the equation \eqref{m-eq:aff_model} is given by a polynomial $f(x)$
    with small rational coefficients, one can still improve the computation
    of $\ytab(u)$ in \eqref{m-eq:ytab_comp} by going back to
    the computation of $y(\xab(u))=f(x)^{\frac1m}$.
    The advantage is that baby-step giant-step splitting can be used for
    the evaluation of $f(x)$, reducing the number of multiplications to
    $O(\sqrt{n})$. In order to recover $\ytab(u)$, one needs to divide by
    $\sqrt[m]{1-u^2}$ and adjust a multiplicative constant including
    the winding number $q(u)$, which can be evaluated at low precision.
    This technique must not be used when $u$ gets close to $\pm1$.

    \subsubsection{Splitting bad integrals or moving integration path}
    \label{subsubsec:splitting}

    Numerical integration becomes quite inefficient when there are other branch points
    relatively close to an edge. The spanning tree optimization does not help
    if some branch points tend to cluster while others are far away. A simple
    example is given by the curve $y^2=x(x-i)(x-1000)$: the
    branch point $i$ is too close to the integration path $[0,1000]$
    and imposes a value $r=0.04$ for Gauss-Chebychev integration and
    a better but still small $r=0.2$ with double-exponential integration.

    In a case like this, one can always split the bad integrals to improve the
    relative distances to the singularities: in the case of double-exponential
    integration, writing
    $\int_0^{1000}=\int_{0}^{6}+\int_6^{1000}$ gives two integrals with
    $r=0.48$ each. Splitting further at $2$ and $33$ gives $r=0.63$.

    Another option with double exponential integration,  as explained in \cite[II.3.5]{Molin2010}, is to shift
    the integration path that is used for the change of variable.

\biblio
\end{document}
